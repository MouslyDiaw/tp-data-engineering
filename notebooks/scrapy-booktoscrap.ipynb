{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337c48bd-5e9a-4f11-8f45-33bd1d8aee9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sommaire\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43de3308-4480-4323-a4e9-c9d88ed71b99",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b0c807d-5fa7-4499-aec2-165e24358440",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-16T09:51:17.575642Z",
     "iopub.status.busy": "2023-06-16T09:51:17.573601Z",
     "iopub.status.idle": "2023-06-16T09:51:21.116957Z",
     "shell.execute_reply": "2023-06-16T09:51:21.115827Z",
     "shell.execute_reply.started": "2023-06-16T09:51:17.574777Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import pendulum\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess, CrawlerRunner\n",
    "from scrapy.utils.log import configure_logging\n",
    "from scrapy.utils.project import get_project_settings\n",
    "from twisted.internet import reactor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b812a2bf-0606-4415-8b1c-640a157ab2c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-16T09:51:54.149975Z",
     "iopub.status.busy": "2023-06-16T09:51:54.149528Z",
     "iopub.status.idle": "2023-06-16T09:51:54.173971Z",
     "shell.execute_reply": "2023-06-16T09:51:54.172824Z",
     "shell.execute_reply.started": "2023-06-16T09:51:54.149931Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrapy.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1128a1c-22d9-438c-acdf-010bd2055a56",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d871d18-904d-4691-9636-ea04ae0609c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-16T09:59:33.136392Z",
     "iopub.status.busy": "2023-06-16T09:59:33.135844Z",
     "iopub.status.idle": "2023-06-16T09:59:33.148998Z",
     "shell.execute_reply": "2023-06-16T09:59:33.147709Z",
     "shell.execute_reply.started": "2023-06-16T09:59:33.136328Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution date: 2023-06-16T11:59:33.139309+02:00 \n",
      "Data URI: /Users/mouslydiaw/Downloads/data_engineer/data \n",
      "Data output path: /Users/mouslydiaw/Downloads/data_engineer/data/2023-06-16-books_data.csv\n"
     ]
    }
   ],
   "source": [
    "DATE_RUN = pendulum.now()\n",
    "OUTPUT_DATA_URI = Path(Path.cwd(), \"data\")\n",
    "\n",
    "FEED_URI = Path(OUTPUT_DATA_URI, f\"{DATE_RUN.to_date_string()}-books_data.csv\")\n",
    "print(f\"Execution date: {DATE_RUN} \\nData URI: {OUTPUT_DATA_URI} \\nData output path: {FEED_URI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04b376b4-0fa9-49e7-b370-38773a09641c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-16T10:00:25.729721Z",
     "iopub.status.busy": "2023-06-16T10:00:25.729237Z",
     "iopub.status.idle": "2023-06-16T10:00:25.735915Z",
     "shell.execute_reply": "2023-06-16T10:00:25.734820Z",
     "shell.execute_reply.started": "2023-06-16T10:00:25.729676Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the OUTPUT_DATA_URI folder if doesn't exist\n",
    "OUTPUT_DATA_URI.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e228e706-d140-483e-95af-c1952898033f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_project_settings de Scrapy permet d'obtenir les paramètres de configuration du projet actuel.\n",
    "# Ces paramètres sont généralement définis dans le fichier settings.py du projet Scrapy.\n",
    "get_project_settings()\n",
    "\n",
    "# cf: https://docs.scrapy.org/en/latest/topics/practices.html for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "606e5b69-6666-4202-9970-0185f64d062e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-16T10:00:53.212422Z",
     "iopub.status.busy": "2023-06-16T10:00:53.212060Z",
     "iopub.status.idle": "2023-06-16T10:00:53.223091Z",
     "shell.execute_reply": "2023-06-16T10:00:53.221732Z",
     "shell.execute_reply.started": "2023-06-16T10:00:53.212388Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-06-16 11:59:33'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATE_RUN.to_datetime_string()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53ab492-7ef3-4cc0-8652-8da4c50a2451",
   "metadata": {},
   "source": [
    "# Définition de la classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8875b8cc-ed33-4247-93c1-f286fcc7b3fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-16T10:02:30.846358Z",
     "iopub.status.busy": "2023-06-16T10:02:30.845946Z",
     "iopub.status.idle": "2023-06-16T10:02:30.860458Z",
     "shell.execute_reply": "2023-06-16T10:02:30.857884Z",
     "shell.execute_reply.started": "2023-06-16T10:02:30.846318Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the data scrappng at: 2023-06-16 12:02:30\n"
     ]
    }
   ],
   "source": [
    "class BooksSpider(scrapy.Spider):\n",
    "    start_date = pendulum.now().to_datetime_string()\n",
    "    print(f\"Starting the data scrappng at: {start_date}\")\n",
    "    name = 'books'\n",
    "    start_urls = [\n",
    "        'https://books.toscrape.com/catalogue/category/books/mystery_3/index.html',\n",
    "        'https://books.toscrape.com/catalogue/category/books/womens-fiction_9/index.html',\n",
    "        'https://books.toscrape.com/catalogue/category/books/psychology_26/index.html',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        category = response.css('div.page-header > h1::text').get()\n",
    "        books = response.css('article.product_pod')\n",
    "        for book in books:\n",
    "            yield {\n",
    "                'title': book.css('h3 > a::attr(title)').get(),  # nom des livre\n",
    "                'price': book.css('div.product_price > p.price_color::text').get(),  # prix\n",
    "                'category': category,  # catégorie de livre\n",
    "                \"calcul_date\": start_date  # execution date\n",
    "            }\n",
    "        # passer à la page suivante\n",
    "        next_page_url = response.css('li.next > a::attr(href)').get()\n",
    "        if next_page_url:\n",
    "            yield response.follow(next_page_url, callback=self.parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4804f200-bbb5-47c9-bc95-4afaa45e1c09",
   "metadata": {},
   "source": [
    "# Extraction et sauvegarde des résultat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60362306-6bad-4bce-a62f-7d925a1b0fd9",
   "metadata": {},
   "source": [
    "⏩ Dans cette partie du code, j'ai créé une instance de la classe CrawlerProcess de Scrapy. Cette classe est responsable de l'exécution des spiders et de la gestion de l'ensemble du processus d'extraction de données.\n",
    "\n",
    "Le paramètre settings passé à la classe CrawlerProcess spécifie les paramètres de configuration pour le processus de crawling. Deux paramètres sont utilisés ici :\n",
    "\n",
    "- \"format\": Cela spécifie le format de sortie des données extraites. Dans notre cas, il est défini sur 'csv' pour que les données soient exportées au format CSV\n",
    "- fields: fields to export\n",
    "- overwrite: flag to know if we replace ou append new data\n",
    "- FEED_URI: Cela spécifie l'emplacement où les données extraites seront stockées. Dans cet exemple, nous l'avons défini sur 'books_data.csv', ce qui signifie que les données seront enregistrées dans un fichier CSV appelé 'books_data.csv'.\n",
    "\n",
    "\n",
    "=> Pour plus de paramètres, cf: https://docs.scrapy.org/en/latest/topics/feed-exports.html#std-setting-FEEDS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5212e9e3-9d36-4257-adb3-4f712594919f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-16T10:02:33.372172Z",
     "iopub.status.busy": "2023-06-16T10:02:33.371725Z",
     "iopub.status.idle": "2023-06-16T10:02:35.697059Z",
     "shell.execute_reply": "2023-06-16T10:02:35.693976Z",
     "shell.execute_reply.started": "2023-06-16T10:02:33.372127Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-16 12:02:33 [scrapy.utils.log] INFO: Scrapy 2.9.0 started (bot: scrapybot)\n",
      "2023-06-16 12:02:33 [scrapy.utils.log] INFO: Versions: lxml 4.8.0.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.9.7 (default, Sep 16 2021, 08:50:36) - [Clang 10.0.0 ], pyOpenSSL 23.2.0 (OpenSSL 3.1.1 30 May 2023), cryptography 41.0.1, Platform macOS-10.16-x86_64-i386-64bit\n",
      "2023-06-16 12:02:33 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n",
      "2023-06-16 12:02:33 [py.warnings] WARNING: /Users/mouslydiaw/opt/anaconda3/envs/python39/lib/python3.9/site-packages/scrapy/utils/request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n",
      "2023-06-16 12:02:33 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2023-06-16 12:02:33 [scrapy.extensions.telnet] INFO: Telnet Password: e154f3cb906a8680\n",
      "2023-06-16 12:02:33 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2023-06-16 12:02:34 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2023-06-16 12:02:34 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2023-06-16 12:02:34 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2023-06-16 12:02:34 [scrapy.core.engine] INFO: Spider opened\n",
      "2023-06-16 12:02:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2023-06-16 12:02:34 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2023-06-16 12:02:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://books.toscrape.com/catalogue/category/books/psychology_26/index.html> (referer: None)\n",
      "2023-06-16 12:02:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://books.toscrape.com/catalogue/category/books/mystery_3/index.html> (referer: None)\n",
      "2023-06-16 12:02:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://books.toscrape.com/catalogue/category/books/womens-fiction_9/index.html> (referer: None)\n",
      "2023-06-16 12:02:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://books.toscrape.com/catalogue/category/books/psychology_26/index.html> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mouslydiaw/opt/anaconda3/envs/python39/lib/python3.9/site-packages/scrapy/utils/defer.py\", line 260, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"/Users/mouslydiaw/opt/anaconda3/envs/python39/lib/python3.9/site-packages/scrapy/utils/python.py\", line 336, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/Users/mouslydiaw/opt/anaconda3/envs/python39/lib/python3.9/site-packages/scrapy/utils/python.py\", line 336, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/Users/mouslydiaw/opt/anaconda3/envs/python39/lib/python3.9/site-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"/Users/mouslydiaw/opt/anaconda3/envs/python39/lib/python3.9/site-packages/scrapy/spidermiddlewares/offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"/Users/mouslydiaw/opt/anaconda3/envs/python39/lib/python3.9/site-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"/Users/mouslydiaw/opt/anaconda3/envs/python39/lib/python3.9/site-packages/scrapy/spidermiddlewares/referer.py\", line 352, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"/Users/mouslydiaw/opt/anaconda3/envs/python39/lib/python3.9/site-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"/Users/mouslydiaw/opt/anaconda3/envs/python39/lib/python3.9/site-packages/scrapy/spidermiddlewares/urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"/Users/mouslydiaw/opt/anaconda3/envs/python39/lib/python3.9/site-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"/Users/mouslydiaw/opt/anaconda3/envs/python39/lib/python3.9/site-packages/scrapy/spidermiddlewares/depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"/Users/mouslydiaw/opt/anaconda3/envs/python39/lib/python3.9/site-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"/var/folders/g2/2f628mld5wlfh2kbczd8rlvc0000gn/T/ipykernel_46002/2778118157.py\", line 19, in parse\n",
      "    \"calcul_date\": start_date  # execution date\n",
      "NameError: name 'start_date' is not defined\n",
      "2023-06-16 12:02:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://books.toscrape.com/catalogue/category/books/mystery_3/index.html> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mouslydiaw/opt/anaconda3/envs/python39/lib/python3.9/site-packages/scrapy/utils/defer.py\", line 260, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"/Users/mouslydiaw/opt/anaconda3/envs/python39/lib/python3.9/site-packages/scrapy/utils/python.py\", line 336, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/Users/mouslydiaw/opt/anaconda3/envs/python39/lib/python3.9/site-packages/scrapy/utils/python.py\", line 336, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/Users/mouslydiaw/opt/anaconda3/envs/python39/lib/python3.9/site-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"/Users/mouslydiaw/opt/anaconda3/envs/python39/lib/python3.9/site-packages/scrapy/spidermiddlewares/offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"/Users/mouslydiaw/opt/anaconda3/envs/python39/lib/python3.9/site-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"/Users/mouslydiaw/opt/anaconda3/envs/python39/lib/python3.9/site-packages/scrapy/spidermiddlewares/referer.py\", line 352, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"/Users/mouslydiaw/opt/anaconda3/envs/python39/lib/python3.9/site-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"/Users/mouslydiaw/opt/anaconda3/envs/python39/lib/python3.9/site-packages/scrapy/spidermiddlewares/urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"/Users/mouslydiaw/opt/anaconda3/envs/python39/lib/python3.9/site-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"/Users/mouslydiaw/opt/anaconda3/envs/python39/lib/python3.9/site-packages/scrapy/spidermiddlewares/depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"/Users/mouslydiaw/opt/anaconda3/envs/python39/lib/python3.9/site-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"/var/folders/g2/2f628mld5wlfh2kbczd8rlvc0000gn/T/ipykernel_46002/2778118157.py\", line 19, in parse\n",
      "    \"calcul_date\": start_date  # execution date\n",
      "NameError: name 'start_date' is not defined\n",
      "2023-06-16 12:02:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://books.toscrape.com/catalogue/category/books/womens-fiction_9/index.html> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mouslydiaw/opt/anaconda3/envs/python39/lib/python3.9/site-packages/scrapy/utils/defer.py\", line 260, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"/Users/mouslydiaw/opt/anaconda3/envs/python39/lib/python3.9/site-packages/scrapy/utils/python.py\", line 336, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/Users/mouslydiaw/opt/anaconda3/envs/python39/lib/python3.9/site-packages/scrapy/utils/python.py\", line 336, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/Users/mouslydiaw/opt/anaconda3/envs/python39/lib/python3.9/site-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"/Users/mouslydiaw/opt/anaconda3/envs/python39/lib/python3.9/site-packages/scrapy/spidermiddlewares/offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"/Users/mouslydiaw/opt/anaconda3/envs/python39/lib/python3.9/site-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"/Users/mouslydiaw/opt/anaconda3/envs/python39/lib/python3.9/site-packages/scrapy/spidermiddlewares/referer.py\", line 352, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "  File \"/Users/mouslydiaw/opt/anaconda3/envs/python39/lib/python3.9/site-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"/Users/mouslydiaw/opt/anaconda3/envs/python39/lib/python3.9/site-packages/scrapy/spidermiddlewares/urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "  File \"/Users/mouslydiaw/opt/anaconda3/envs/python39/lib/python3.9/site-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"/Users/mouslydiaw/opt/anaconda3/envs/python39/lib/python3.9/site-packages/scrapy/spidermiddlewares/depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "  File \"/Users/mouslydiaw/opt/anaconda3/envs/python39/lib/python3.9/site-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"/var/folders/g2/2f628mld5wlfh2kbczd8rlvc0000gn/T/ipykernel_46002/2778118157.py\", line 19, in parse\n",
      "    \"calcul_date\": start_date  # execution date\n",
      "NameError: name 'start_date' is not defined\n",
      "2023-06-16 12:02:35 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2023-06-16 12:02:35 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 800,\n",
      " 'downloader/request_count': 3,\n",
      " 'downloader/request_method_count/GET': 3,\n",
      " 'downloader/response_bytes': 127447,\n",
      " 'downloader/response_count': 3,\n",
      " 'downloader/response_status_count/200': 3,\n",
      " 'elapsed_time_seconds': 0.846527,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2023, 6, 16, 10, 2, 35, 578767),\n",
      " 'log_count/DEBUG': 4,\n",
      " 'log_count/ERROR': 3,\n",
      " 'log_count/INFO': 10,\n",
      " 'log_count/WARNING': 1,\n",
      " 'memusage/max': 183037952,\n",
      " 'memusage/startup': 183037952,\n",
      " 'response_received_count': 3,\n",
      " 'scheduler/dequeued': 3,\n",
      " 'scheduler/dequeued/memory': 3,\n",
      " 'scheduler/enqueued': 3,\n",
      " 'scheduler/enqueued/memory': 3,\n",
      " 'spider_exceptions/NameError': 3,\n",
      " 'start_time': datetime.datetime(2023, 6, 16, 10, 2, 34, 732240)}\n",
      "2023-06-16 12:02:35 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "process = CrawlerProcess(\n",
    "    settings={\n",
    "        \"FEEDS\": {\n",
    "            FEED_URI: {\n",
    "                # format data export\n",
    "                \"format\": \"csv\",\n",
    "                # define the fields to export, their order and their output names\n",
    "                \"fields\": [\"title\", \"price\", \"category\"],\n",
    "                # overwrite the file if it already exists (True) or append to its content (False).\n",
    "                \"overwrite\": False,\n",
    "            }\n",
    "        }\n",
    "})\n",
    "\n",
    "process.crawl(BooksSpider)\n",
    "process.start(stop_after_crawl=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a879d9b-2f1a-4d1a-9e54-5f007ee0654b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa5bdb25-5d8f-4f45-8a97-a110d71cf1b2",
   "metadata": {},
   "source": [
    "## Vérification des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69aecf26-47f2-4f3f-82e9-bbfe28f573bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DATA_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5c18bc6-46ed-4d5e-b513-c7469b2dd39a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-16T10:00:37.128990Z",
     "iopub.status.busy": "2023-06-16T10:00:37.128243Z",
     "iopub.status.idle": "2023-06-16T10:00:37.283199Z",
     "shell.execute_reply": "2023-06-16T10:00:37.278579Z",
     "shell.execute_reply.started": "2023-06-16T10:00:37.128943Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/mouslydiaw/Downloads/data_engineer/data/2023-06-16-books_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data_book \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mFEED_URI\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m data_book\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/python39/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/python39/lib/python3.9/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/python39/lib/python3.9/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/python39/lib/python3.9/site-packages/pandas/io/parsers/readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/python39/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/python39/lib/python3.9/site-packages/pandas/io/common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    788\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/mouslydiaw/Downloads/data_engineer/data/2023-06-16-books_data.csv'"
     ]
    }
   ],
   "source": [
    "data_book = pd.read_csv(FEED_URI, sep=\",\")\n",
    "data_book.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3221fde2-b504-49ad-a159-892b330e7aaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
